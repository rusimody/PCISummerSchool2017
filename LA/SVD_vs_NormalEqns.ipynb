{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A Numerical Example (Least Squares)\n",
    "\n",
    "- We create a data matrix A with one column that is virtually dependent on the other two: by a small random vector (so as to make A's condition number reasonably large).\n",
    "- we create a target column (the dependent vector in linear least squares) that differs from a linear combination of two data columns by a small random vector: the latter determines how far the target is from the range of A and hence, how large the residual is from the least squares solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "c1 = np.array([1,2,4,8])\n",
    "c2 = np.array([3,6,9,12])\n",
    "c3 = c1 - 4*c2 + 0.0000001*(np.random.rand(4) - 0.5*np.ones(4).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.matrix([c1,c2,c3]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  1.        ,   3.        , -10.99999998],\n",
       "        [  2.        ,   6.        , -22.00000001],\n",
       "        [  4.        ,   9.        , -32.00000005],\n",
       "        [  8.        ,  12.        , -40.00000004]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A  # Matrix with data columns; column 3 is almost dependent on columns 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = 2*c1 - 7*c2 + 0.0001*(np.random.rand(4) - 0.5*np.ones(4))    # Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.matrix(b).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u, sigma, vt = np.linalg.svd(A)  # the singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.19098134,  0.27860381,  0.84834319, -0.40770072],\n",
       "        [-0.38196268,  0.55720763,  0.08310079,  0.73261069],\n",
       "        [-0.55954094,  0.38729987, -0.50727238, -0.52876033],\n",
       "        [-0.71031301, -0.67963106,  0.12681809,  0.13219008]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number of $A$ is $\\sigma_1 / \\sigma_3 \\approx 10^{9}$. \n",
    "\n",
    "Hence, the condition number of $A^T A$, the ratio of its largest to smallest eigenvalue, is $\\approx 10^{18}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.98101360e+01,   2.59762139e+00,   8.64308580e-09])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma   # note disparity in the relative magitudes of the largest vs. smallest singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  1.        ,   3.        , -10.99999998],\n",
       "        [  2.        ,   6.        , -22.00000001],\n",
       "        [  4.        ,   9.        , -32.00000005],\n",
       "        [  8.        ,  12.        , -40.00000004]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u[:,:3]*(np.diag(sigma))*vt   # Full reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigmap = np.linalg.pinv(np.diag(sigma))  # Moore-Penrose inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigmap = np.matrix(sigmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  1.67195741e-02,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   3.84967572e-01,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   1.15699418e+08]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.14839583, -0.96042861, -0.23570226],\n",
       "        [-0.27460805, -0.18894849,  0.94280904],\n",
       "        [ 0.95003637, -0.20463466,  0.23570226]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the linear least squares regression problem using the SVD of $A$, namely $A = U \\Sigma V^T$:\n",
    "\n",
    "$ x = V {\\Sigma}^{+} U^T b$\n",
    "\n",
    "where ${\\Sigma}^{+}$ is the Moore-Penrose inverse of the diagonal matrix $\\Sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r1 = b - A*(vt.T)*sigmap*(u[:,:3].T)*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ -8.43690507e-06],\n",
       "        [  1.51605501e-05],\n",
       "        [ -1.09421045e-05],\n",
       "        [  2.73551808e-06]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0693872391528605e-05"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the SVD approach yields a reasonable answer: we should expect a very close fit to the target $b$ and hence a very small residual error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.33795267e-13,   6.74763688e+00,   3.57725237e+03])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eigvalsh(A.T*A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now solve the linear least squares regression problem using the formula derived from the normal equations:\n",
    "\n",
    "$ x = (A^T A)^{-1} A^T b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2 = A*np.linalg.inv(A.T*A)*(A.T)*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  5.43090088],\n",
       "        [ 10.86178517],\n",
       "        [ 16.76734979],\n",
       "        [-62.70597124]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.035262618442502"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(r2 - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the wild answer! This is due to the numerical instability of computing the inverse of $A^T A$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
